##Slide 0##

Hello, I am Kate and today I want to tell you about history of neural networks.

Neural network is an attempt using mathematical models to reproduce the work of the human brain to create machines with artificial intelligence.
  
  
  
  
##Slide 1##

A neural network is a sequence of connected neurons.

Neurons are units that receive and transmit information.

Neurons matter only in the chain that is built from them.

The neuron receives incoming signals and all of them is assigned a certain weight.

The signal is multiplied by its weight, the values are added up to each other, and as a result single number is obtained, which is finally received with the activation function.

At the output function decides it is need to transmit the signal further.

A simple neural network can consist of three levels and transmit data only forward.
  
  
  
  
##Slide 2##

The term “neural network” appeared in the middle of the 20th century.

The first works in this field were done by McCallock and Pitts.


##next slide##


In 1943, they developed a computer model of a neural network, based on mathematical algorithms and the theory of brain activity. 

They suggested that neurons can be simplistically viewed as devices that operate with binary numbers, and called this model “threshold logic”.
  
  
  
  
##Slide 3##

In 1949, Canadian physiologist and psychologist Hebb expressed ideas about the nature of the connection of brain neurons and their interaction.

##next slide##

According to Hebb, a result of frequent stimulation of the nervous system, coordinated neural structures are formed cell assemblies. 

Cell ensembles develop as a result of stimulation of neuron connections with each other.

These processes occurring in the brain are the biological basis of the learning process.
  
  
  
  
##Slide 4##

In the 1954 year two researchers from the Massachusetts Institute of Technology, Farley and Clark, successfully simulated a network of 128 neurons on a computer that could recognize simple models after the training phase.
  
  
  
  
##Slide 5##

In the summer of 1956, a two-month scientific seminar on artificial intelligence was held at Dartmouth College.

##next slide##

The event was important for the history of the trend: it brought together people who is interested in modeling the human mind, approved the main provisions of the new field of science

##next slide##

and gave the name “artificial intelligence”.
  
  
  
  
##Slide 6##

In 1957, Rosenblatt developed a mathematical and computer model of brain information perception based on a two-layer learning neural network.

In training, this network used arithmetic operations of addition and subtraction.

He called this model a “perceptron”.

##next slide##

In 1958, he proposed a model of an electronic device that was supposed to imitate the processes of human thinking, and two years later he demonstrated the first working machine, which could learn to recognize some of the letters written on cards that brought to his “eyes”.
  
  
  
  
##Slide 8##

Research on neural networks slowed down to the time when computers reached great computing power. 

One of the important steps that stimulated further research was the development in 1974 by Galushkin, as well as independently by Verbos, of the error back propagation method, which made it possible to effectively solve the problem of training multilayer networks and solve the problem of “Exclusive OR”.

##next slide##

Learning by the error back propagation algorithm involves two passes through all layers of the network: forward and backward.
  
  
  
  
##Slide 9##

In 1975, Fukushima developed the cognitron, which became one of the first multilayer neural networks.

##next slide##

The Cognitron is constructed in the form of layers of neurons connected by synapses.

Each neuron is connected only with neurons in an adjacent.
  
  
  
  
##Slide 10##

It was possible to achieve two-way information transfer between neurons only in the Hopfield network, and the specialization of these nodes for specific purposes was introduced in the first hybrid networks.

##next slide##

Each neuron is connected by synapses with all other neurons, and also has one input synapse.

So, this network is a recursive network, and has feedbacks. Network functions are cyclical.
  
  
  
  
##Slide 11##

Kohonen presents models of a network that learns without a teacher (Kohonen neural network), solves the problems of clustering, data visualization (Kohonen self-organizing map) and other tasks of preliminary data analysis.

##next slide##

In the Kohonen network, each neuron of the input layer is connected to all neurons of the output layer, but there are no connections inside the layers. 
  
  
  
  
##Slide 12##

The parallel distributed data processing algorithm in the mid-1980s became popular under the name of connectivism. In 1986, in the work of Rummelhart and McClelland, connectivity was used for computer simulation of neural processes.

Connectivism is based on theories of chaos, network, complexity and self-organization.

Connectivism assumes that decisions are made on the basis of rapidly changing initial conditions.
  
  
  
  
##Slide 13##

in 2006, several uncontrolled training procedures for neural networks with one or more layers were proposed using the so-called deep learning algorithms.

As in many other cases, tasks of high complexity require the use of not one, but several methods of solution or their synthesis.

So, that’s all information that I want to bring to you.
Thanks for your attention!