##Слайд 0##


Нейронная сеть — попытка с помощью математических моделей воспроизвести работу человеческого мозга для создания машин, обладающих искусственным интеллектом.


[ссылка](https://neurohive.io/ru/osnovy-data-science/osnovy-nejronnyh-setej-algoritmy-obuchenie-funkcii-aktivacii-i-poteri/)
  
  

##Слайд 1##


Нейронная сеть представляет собой последовательность связанных нейронов.

Нейроны — единицы, получающие и передающие информацию. Сами по себе они не играют важной роли: нейроны имеют значение только в выстроенной из них цепи.

К нейрону поступают входящие сигналы, каждому из которых присвоен определенный вес. Сигнал умножается на свой вес, значения суммируются, и получается единое число, которое получает активационная функция. На выходе она принимает решение, транслировать ли сигнал дальше.

Простая нейронная сеть может состоять из трех уровней и передавать данные только вперед. Она включает в себя входящие нейроны, скрытый (промежуточный) слой нейронов, недоступный внешнему обозревателю, и нейрон на выходе.


[ссылка](https://robo-hunter.com/news/kak-rabotayt-neironnie-seti-o-slojnoi-sisteme-prostimi-slovami14200)
  


##Слайд 2##


Термин «нейронная сеть» появился в середине XX века. Первые работы, в которых были получены основные результаты в данном направлении, были проделаны Мак-Каллоком и Питтсом. В 1943 году ими была разработана компьютерная модель нейронной сети на основе математических алгоритмов и теории деятельности головного мозга. Они выдвинули предположение, что нейроны можно упрощённо рассматривать как устройства, оперирующие двоичными числами, и назвали эту модель «пороговой логикой».

Исследователи предложили конструкцию сети из электронных нейронов и показали, что подобная сеть может выполнять практически любые вообразимые числовые или логические операции. Мак-Каллок и Питтс предположили, что такая сеть в состоянии также обучаться, распознавать образы, обобщать, т. е. обладает всеми чертами интеллекта.

Данная модель заложила основы двух различных подходов исследований нейронных сетей. Один подход был ориентирован собственно на изучение биологических процессов в головном мозге, другой – на применение нейронных сетей как метода искусственного интеллекта для решения различных прикладных задач.


[ссылка](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[ссылка](https://newtonew.com/hero/history-neuroscience-pitts)
  


##Слайд 3##


В 1949 году канадский физиолог и психолог Хебб высказал идеи о характере соединения нейронов мозга и их взаимодействии.

«When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased»

«Если аксон клетки А находится достаточно близко, чтобы возбуждать клетку B, и неоднократно или постоянно принимает участие в ее возбуждении, то наблюдается некоторый процесс роста или метаболических изменений в одной или обеих клетках, ведущий к увеличению эффективности А, как одной из клеток возбуждающих В»

Другими словами, по мнению Хебба, в результате частой стимуляции нервной системы формируются скоординированные нейронные структуры — ансамбли клеток (cell assemblies). Ансамбли клеток развиваются в результате стимуляции связей нейронов друг с другом. Эти процессы, происходящие в головном мозге, являются биологической основой процесса обучения.


[ссылка](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[ссылка](https://habr.com/ru/post/102305/)

[ссылка](https://ru.wikipedia.org/wiki/%D0%A5%D0%B5%D0%B1%D0%B1,_%D0%94%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%B4)
  


##Слайд 4##


Внезапно оборвавшаяся в 1954 году жизнь Алана Тьюринга не позволила ему закончить исследования в Манчестерском университете. Он как раз приступил к разработке моделей нейронных цепей, с помощью которых можно изучать так называемые «умные» машины, учитывая особенности работы человеческого мозга. В год смерти Тьюринга двое исследователей из Массачусетского технологического института, Бельмонт Фарли (1920-2008) и Уэсли Кларк (р. 1927), успешно смоделировали на компьютере сеть из 128 нейронов, которые могли распознавать простые модели после фазы обучения. Ученые отметили, что при уменьшении количества нейронов на 10% сеть не теряла способностей к распознаванию.

Конечно, модель была элементарной, она состояла из нейронов, соединенных друг с другом случайным образом, каждое соединение было связано с определенным весом, и нейронная цепь вела себя подобно сети Маккалока — Питтса. Ее обучение происходило в соответствии с правилом Хебба, то есть когда один нейрон постоянно стимулировал другой, их синаптическая пластичность возрастала, и вес соединения между обоими нейронами увеличивался.


[ссылка](https://www.rulit.me/books/razmyshleniya-o-dumayushchih-mashinah-tyuring-kompyuternoe-ischislenie-read-455804-25.html)

[ссылка](https://www.computerhistory.org/collections/catalog/102711957)
  


##Слайд 5##


Летом 1956 года в Дартмутском колледже был проведен двухмесячный научный семинар по вопросам искусственного интеллекта.

Мероприятие имело важное значение для истории направления: на нем встретились люди, интересующиеся вопросами моделирования человеческого разума, были утверждены основные положения новой области науки и дано наименование англ. «artificial intelligence» (термин был предложен Джоном Маккарти).

Пытаясь проникнуть в суть работы мозга, ученые пришли к выводу, что на данный момент не установлено, каким образом активность нейронов способствует решению задач. Перенося эту проблему на программирование, становится понятной необходимость создания нелинейного механизма решения задач путём обучения машины создавать и манипулировать концептами, абстракциями.

Дартмутский семинар не стал местом каких-либо новых крупных открытий, но именно он позволил сойтись вместе и познакомиться всем наиболее важным деятелям в этой научной области.


[ссылка](https://ru.wikipedia.org/wiki/%D0%94%D0%B0%D1%80%D1%82%D0%BC%D1%83%D1%82%D1%81%D0%BA%D0%B8%D0%B9_%D1%81%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80)

[ссылка](https://en.wikipedia.org/wiki/Dartmouth_workshop)
  


##Слайд 6##


В 1957 году Розенблаттом были разработаны математическая и компьютерная модели восприятия информации мозгом на основе двухслойной обучающейся нейронной сети. При обучении данная сеть использовала арифметические действия сложения и вычитания. Такую модель он назвал “перцептрон”.

Надо отметить, что Розенблатт разработал не какой-то один вид искусственной нейронной сети. Он разработал полную классификацию всевозможных нейронных сетей.

В 1958 году им была предложена модель электронного устройства, которое должно было имитировать процессы человеческого мышления, а два года спустя была продемонстрирована первая действующая машина, которая могла научиться распознавать некоторые из букв, написанных на карточках, которые подносили к его «глазам», напоминающим кинокамеры.


[ссылка](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[ссылка](https://habr.com/ru/post/140301/)
  

 
##Слайд 7##


Зима искусственного интеллекта

Интерес к исследованию нейронных сетей угас после публикации работы по машинному обучению Минского и Пейперта в 1969 году. Ими были обнаружены основные вычислительные проблемы, возникающие при компьютерной реализации искусственных нейронных сетей. Первая проблема состояла в том, что однослойные нейронные сети не могли совершать «сложение по модулю 2», то есть реализовать функцию «Исключающее ИЛИ». Второй важной проблемой было то, что компьютеры не обладали достаточной вычислительной мощностью, чтобы эффективно обрабатывать огромный объём вычислений, необходимый для больших нейронных сетей.


[ссылка](https://ru.wikipedia.org/wiki/%D0%97%D0%B8%D0%BC%D0%B0_%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D0%B0)

[ссылка](https://en.wikipedia.org/wiki/AI_winter)
  

 
##Слайд 8##


Исследования нейронных сетей замедлились до того времени, когда компьютеры достигли больших вычислительных мощностей. Одним из важных шагов, стимулировавших дальнейшие исследования, стала разработка  в 1974 г. А. И. Галушкиным, а также независимо и одновременно Полом Дж. Вербосом метода обратного распространения ошибки, который позволил эффективно решать задачу обучения многослойных сетей и решить проблему со «сложением по модулю 2».

Обучение алгоритмом обратного распространения ошибки предполагает два прохода по всем слоям сети: прямого и обратного. При прямом проходе входной вектор подается на входной слой нейронной сети, после чего распространяется по сети от слоя к слою. В результате генерируется набор выходных сигналов, который и является фактической реакцией сети на данный входной образ. Во время прямого прохода все синаптические веса сети фиксированы. Во время обратного прохода все синаптические веса настраиваются в соответствии с правилом коррекции ошибок, а именно: фактический выход сети вычитается из желаемого, в результате чего формируется сигнал ошибки. Этот сигнал впоследствии распространяется по сети в направлении, обратном направлению синаптических связей.
Синаптические веса настраиваются с целью максимального приближения выходного сигнала сети к желаемому.


[ссылка](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[ссылка](http://www.aiportal.ru/articles/neural-networks/back-propagation.html)
  


##Слайд 9##


В 1975 году Фукусимой был разработан когнитрон, который стал одной из первых многослойных нейронных сетей.
Когнитрон конструируется в виде слоев нейронов, соединенных синапсами. предсинаптический нейрон в одном слое связан с постсинаптическим нейроном в следующем слое. Имеются два типа нейронов: возбуждающие узлы, которые стремятся вызвать возбуждение постсинаптического узла, и тормозящие узлы, которые тормозят это возбуждение. Возбуждение нейрона определяется взвешенной суммой его возбуждающих и тормозящих входов, однако в действительности механизм является более сложным, чем простое суммирование.

Kаждый нейрон связан только с нейронами в соседней области, называемой областью связи. Это ограничение области связи согласуется с анатомией зрительной коры, в которой редко соединяются между собой нейроны, располагающиеся друг от друга на расстоянии более одного миллиметра.

Фактическая структура сети и методы, используемые в когнитроне для настройки относительных весов связей, варьировались от одной стратегии к другой. Каждая из стратегий имела свои преимущества и недостатки. Сети могли распространять информацию только в одном направлении или перебрасывать информацию из одного конца в другой, пока не активировались все узлы и сеть не приходила в конечное состояние.


[ссылка](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[ссылка](http://techn.sstu.ru/kafedri/%D0%BF%D0%BE%D0%B4%D1%80%D0%B0%D0%B7%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F/1/MetMat/Terin/neiro/neiro.htm)
  

 
##Слайд 10##


Достичь двусторонней передачи информации между нейронами удалось лишь в сети Хопфилда (1982), и специализация этих узлов для конкретных целей была введена в первых гибридных сетях.

Структурная схема сети Хопфилда состоит из единственного слоя нейронов, число которых является одновременно числом входов и выходов сети. Каждый нейрон связан синапсами со всеми остальными нейронами, а также имеет один входной синапс, через который осуществляется ввод сигнала.

Таким образом данная сеть является рекурсивной сетью, и обладает обратными связями. Функции сети являются циклическими.


[ссылка](http://www.codenet.ru/progr/alg/ai/htm/gl3_5.php)

[ссылка](https://ai-science.ru/set-xopfilda/)
  


##Слайд 11##


Кохоненом представлены модели сети, обучающейся без учителя (нейронная сеть Кохонена), решающей задачи кластеризации, визуализации данных (самоорганизующаяся карта Кохонена) и другие задачи предварительного анализа данных.

Кластер — объединение нескольких однородных элементов, которое может рассматриваться как самостоятельная единица, обладающая определёнными свойствами.

В результате в каждом кластере будут находиться объекты, похожие по своим свойствам друг на друга и отличающиеся от объектов, которые содержатся в других кластерах. При этом чем больше подобие объектов внутри кластера и чем сильнее их отличие от объектов в других кластерах, тем лучше кластеризация.

Сеть Кохонена представляет собой специальный тип нейронной сети для решения задачи кластеризации. Она состоит всего из двух слоев – входного (распределительного) и выходного, который также называют слоем Кохонена.
В сети Кохонена каждый нейрон входного слоя связан со всеми нейронами выходного, а внутри слоев связей нет. На нейроны входного слоя подаются векторы признаков кластеризуемых объектов.

Как и в обычной нейронной сети, входные нейроны не участвуют в процессе обучения и обработки данных, а просто распределяют входной сигнал по нейронам следующего слоя.


[ссылка](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)

[ссылка](https://wiki.loginom.ru/articles/kohonen-network.html)

[ссылка](https://wiki.loginom.ru/articles/clustering.html)

[ссылка](https://ru.wikipedia.org/wiki/%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80)
  
  

##Слайд 12##


Алгоритм параллельной распределённой обработки данных в середине 1980 годов стал популярен под названием коннективизма. В 1986 году в работе Руммельхарта и Мак-Клелланда коннективизм был использован для компьютерного моделирования нейронных процессов.

Коннективизм основывается на теориях хаоса, сети, сложности и самоорганизации.

Коннективизм исходит из того что решения принимаются на основе быстро меняющихся начальных условий. Новые знания постоянно приобретается и жизненно важно различать разницу между важными и неважными знаниями. Важным является умение видеть, когда новые знания изменяет ландшафт на основании решений, принятых вчера.

Принципы коннективизма :

+ Обучение и знания требуют разнообразия подходов и возможности выбрать оптимальный подход.
+ Обучение - это процесс формирования сети подключения специализированных узлов и источников информации.
+ Обучение и познание происходят постоянно - это всегда процесс и никогда состояние.
+ Своевременность (точность, обновляемость знаний) необходимая черта современного обучения. На смену папкам пришли потоки (папки- накопление знаний, потоки - наблюдение за процессами).
+ Обучение является принятием решений. 


[ссылка](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[ссылка](http://cdoippo.blogspot.com/2015/03/blog-post.html)
  


##Слайд 13##


Несмотря на большой энтузиазм, вызванный в научном сообществе разработкой метода обратного распространения ошибки, это также породило многочисленные споры о том, может ли такое обучение быть на самом деле реализовано в головном мозге. Отчасти это связывали с тем, что механизм обратного прохождения сигнала не был очевидным в то время, так как не было явного источника обучающего и целевого сигналов. Тем не менее, в 2006 году было предложено несколько неконтролируемых процедур обучения нейронных сетей с одним или несколькими слоями с использованием так называемых алгоритмов глубокого обучения. Эти алгоритмы могут быть использованы для изучения промежуточных представлений, как с выходным сигналом, так и без него, чтобы понять основные особенности распределения сенсорных сигналов, поступающих на каждый слой нейронной сети.

Как и во многих других случаях, задачи высокой сложности требуют применения не одного, а нескольких методов решения или их синтеза. Не исключение и искусственные нейронные сети. С самого начала нынешнего столетия в работах различных исследователей активно описываются нейро-нечёткие сети, ячеечно-нейросетевые модели. Также нейронные сети используются, например, для настройки параметров нечётких систем управления. В общем, нет никаких сомнений и в дальнейшей интеграции методов искусственного интеллекта между собой и с другими методами решения задач.


[ссылка](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)