##Slide 1##


Neural network - an attempt using mathematical models to reproduce the work of the human brain to create machines with artificial intelligence.


[link](https://neurohive.io/ru/osnovy-data-science/osnovy-nejronnyh-setej-algoritmy-obuchenie-funkcii-aktivacii-i-poteri/)
  


##Slaid 2##


A neural network is a sequence of connected neurons.

Neurons are units that receive and transmit information. By themselves, they do not play an important role: neurons matter only in the chain built from them.

The neuron receives incoming signals, each of which is assigned a certain weight. The signal is multiplied by its weight, the values are added up, and a single number is obtained, which the activation function receives. At the output, she decides whether to broadcast the signal further.

A simple neural network can consist of three levels and transmit data only forward. It includes incoming neurons, a hidden (intermediate) layer of neurons inaccessible to an external browser, and an output neuron.


[link](https://robo-hunter.com/news/kak-rabotayt-neironnie-seti-o-slojnoi-sisteme-prostimi-slovami14200)
  


##Slide 3##


The term “neural network” appeared in the middle of the 20th century. The first works in which the main results were obtained in this direction were done by McCallock and Pitts. In 1943, they developed a computer model of a neural network based on mathematical algorithms and the theory of brain activity. They hypothesized that neurons can be simplistically viewed as devices that operate with binary numbers, and called this model “threshold logic”.

Researchers have proposed the construction of a network of electronic neurons and have shown that such a network can perform almost any imaginable numerical or logical operation. McCallock and Pitts suggested that such a network is also able to learn, recognize images, generalize, i.e., has all the features of intelligence.

This model laid the foundation for two different neural network research approaches. One approach was focused on the study of biological processes in the brain, the other on the use of neural networks as a method of artificial intelligence for solving various applied problems.


[link](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[link](https://newtonew.com/hero/history-neuroscience-pitts)
  


##Slide 4##


In 1949, Canadian physiologist and psychologist Hebb expressed ideas about the nature of the connection of brain neurons and their interaction.

"When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B , is increased »

In other words, according to Hebb, as a result of frequent stimulation of the nervous system, coordinated neural structures are formed - cell assemblies. Cell ensembles develop as a result of stimulation of neuron connections with each other. These processes occurring in the brain are the biological basis of the learning process.


[link](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[link](https://habr.com/ru/post/102305/)

[link](https://ru.wikipedia.org/wiki/%D0%A5%D0%B5%D0%B1%D0%B1,_%D0%94%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%B4)
  


##Slide 5##


Suddenly breaking off in 1954, Alan Turing's life did not allow him to finish his studies at the University of Manchester. He just started developing models of neural circuits, with which you can study the so-called "smart" machines, taking into account the features of the human brain. In the year of Turing's death, two researchers from the Massachusetts Institute of Technology, Belmont Farley (1920-2008) and Wesley Clark (b. 1927), successfully simulated a network of 128 neurons on a computer that could recognize simple models after the training phase. Scientists noted that with a decrease in the number of neurons by 10%, the network did not lose recognition ability.

Of course, the model was elementary, it consisted of neurons randomly connected to each other, each connection was associated with a certain weight, and the neural circuit behaved like a McCulloch-Pitts network. Her training was in accordance with the Hebb rule, that is, when one neuron constantly stimulated another, their synaptic plasticity increased, and the weight of the connection between both neurons increased.


[link](https://www.rulit.me/books/razmyshleniya-o-dumayushchih-mashinah-tyuring-kompyuternoe-ischislenie-read-455804-25.html)

[link](https://www.computerhistory.org/collections/catalog/102711957)
  


##Slide 6##


In the summer of 1956, a two-month scientific seminar on artificial intelligence was held at Dartmouth College.
The event was important for the history of the trend: it brought together people interested in modeling the human mind, approved the main provisions of the new field of science and gave the name “artificial intelligence” (the term was proposed by John McCarthy).

Trying to penetrate the essence of the brain, scientists came to the conclusion that it has not yet been established how the activity of neurons contributes to solving problems. Transferring this problem to programming, it becomes clear the need to create a non-linear mechanism for solving problems by teaching the machine how to create and manipulate concepts, abstractions.

The Dartmouth seminar did not become the site of any major new discoveries, but it was he who made it possible to come together and meet all the most important figures in this scientific field.


[link](https://ru.wikipedia.org/wiki/%D0%94%D0%B0%D1%80%D1%82%D0%BC%D1%83%D1%82%D1%81%D0%BA%D0%B8%D0%B9_%D1%81%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80)

[link](https://en.wikipedia.org/wiki/Dartmouth_workshop)
  


##Slide 7##


In 1957, Rosenblatt developed a mathematical and computer model of brain information perception based on a two-layer learning neural network. In training, this network used arithmetic operations of addition and subtraction. He called this model a “perceptron”.

It should be noted that Rosenblatt did not develop just one kind of artificial neural network. He developed a complete classification of all kinds of neural networks.

In 1958, he proposed a model of an electronic device that was supposed to imitate the processes of human thinking, and two years later he demonstrated the first working machine, which could learn to recognize some of the letters written on cards that brought to his “eyes” resembling movie cameras.


[link](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[link](https://habr.com/ru/post/140301/)
  


##Slide 8##


Winter Artificial Intelligence

Interest in the study of neural networks faded after the publication of machine learning work by Minsky and Peypert in 1969. They discovered the main computational problems that arise in the computer implementation of artificial neural networks. The first problem was that single-layer neural networks could not perform “modulo 2 addition”, that is, implement the “Exclusive OR” function. The second important problem was that computers did not have enough processing power to efficiently handle the huge amount of computation needed for large neural networks.


[link](https://ru.wikipedia.org/wiki/%D0%97%D0%B8%D0%BC%D0%B0_%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D0%B0)

[link](https://en.wikipedia.org/wiki/AI_winter)
  


##Slide 9##


Research on neural networks slowed down to the time when computers reached great computing power. One of the important steps that stimulated further research was the development in 1974 by A. I. Galushkin, as well as independently and simultaneously by Paul J. Verbos, of the error back propagation method, which made it possible to effectively solve the problem of training multilayer networks and solve the problem of “addition by module 2 ".

Learning by the error back propagation algorithm involves two passes through all layers of the network: forward and backward. With a direct pass, the input vector is fed to the input layer of the neural network, after which it propagates through the network from layer to layer. As a result, a set of output signals is generated, which is the actual response of the network to a given input image. During a direct pass, all synaptic net weights are fixed. During the return pass, all synaptic weights are adjusted in accordance with the error correction rule, namely: the actual network output is subtracted from the desired one, as a result of which an error signal is generated. This signal subsequently propagates through the network in the direction opposite to the direction of synaptic connections.

Synaptic weights are adjusted to maximize the network output to the desired.


[link](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[link](http://www.aiportal.ru/articles/neural-networks/back-propagation.html)
  


##Slide 10##


In 1975, Fukushima developed the cognitron, which became one of the first multilayer neural networks.
The Cognitron is constructed in the form of layers of neurons connected by synapses. the presynaptic neuron in one layer is connected with the postsynaptic neuron in the next layer. There are two types of neurons: excitatory nodes, which tend to cause excitation of the postsynaptic node, and inhibitory nodes, which inhibit this excitation. The excitation of a neuron is determined by the weighted sum of its exciting and inhibitory inputs, but in reality the mechanism is more complex than a simple summation. 

each neuron is associated only with neurons in an adjacent region called the communication region. This limitation of the communication region is consistent with the anatomy of the visual cortex, in which neurons rarely spaced from each other at a distance of more than one millimeter are rarely connected.

The actual network structure and methods used in the cognitron to adjust the relative weights of the bonds varied from one strategy to another. Each of the strategies had its advantages and disadvantages. Networks could spread information only in one direction or transfer information from one end to the other until all nodes were activated and the network did not reach its final state.


[link](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[link](http://techn.sstu.ru/kafedri/%D0%BF%D0%BE%D0%B4%D1%80%D0%B0%D0%B7%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F/1/MetMat/Terin/neiro/neiro.htm)
  


##Slide 11##

It was possible to achieve two-way information transfer between neurons only in the Hopfield network (1982), and the specialization of these nodes for specific purposes was introduced in the first hybrid networks.

The structural diagram of the Hopfield network consists of a single layer of neurons, the number of which is simultaneously the number of inputs and outputs of the network. Each neuron is connected by synapses with all other neurons, and also has one input synapse through which the signal is input.

Thus, this network is a recursive network, and has feedbacks. Network functions are cyclical.


[link](http://www.codenet.ru/progr/alg/ai/htm/gl3_5.php)

[link](https://ai-science.ru/set-xopfilda/)
  


##Slide 12##


Kohonen presents models of a network that learns without a teacher (Kohonen neural network), solves the problems of clustering, data visualization (Kohonen self-organizing map) and other tasks of preliminary data analysis.

Cluster is a combination of several homogeneous elements, which can be considered as an independent unit with certain properties.

As a result, in each cluster there will be objects that are similar in their properties to each other and differ from objects that are contained in other clusters. Moreover, the greater the similarity of objects within a cluster and the stronger their difference from objects in other clusters, the better clustering
The Kohonen network is a special type of neural network for solving the clustering problem. It consists of only two layers - the input (distribution) and the output, which is also called the Kohonen layer.

In the Kohonen network, each neuron of the input layer is connected to all neurons of the output layer, but there are no connections inside the layers. Neurons of the input layer are fed with feature vectors of clustered objects.

As in a normal neural network, input neurons do not participate in the process of training and data processing, but simply distribute the input signal to the neurons of the next layer.


[link](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)

[link](https://wiki.loginom.ru/articles/kohonen-network.html)

[link](https://wiki.loginom.ru/articles/clustering.html)

[link](https://ru.wikipedia.org/wiki/%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80)
  


##Slide 13##


The parallel distributed data processing algorithm in the mid-1980s became popular under the name of connectivism. In 1986, in the work of Rummelhart and McClelland, connectivity was used for computer simulation of neural processes.

Connectivism is based on theories of chaos, network, complexity and self-organization.

Connectivism assumes that decisions are made on the basis of rapidly changing initial conditions. New knowledge is constantly being acquired and it is vital to distinguish the difference between important and unimportant knowledge. The ability to see when new knowledge changes the landscape based on decisions made yesterday is important.

The basic principles of connectivism:

+ Training and knowledge require a variety of approaches and the ability to choose the best approach.
+ Training is the process of forming a network for connecting specialized nodes and information sources.
+ Learning and cognition are ongoing - it is always a process and never a state.
+ Timeliness (accuracy, updating knowledge) is a necessary feature of modern education. Folders were replaced by streams (folders - accumulation of knowledge, streams - monitoring processes).
+ Learning is decision making.


[link](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)

[link](http://cdoippo.blogspot.com/2015/03/blog-post.html)
  


##Slide 14##


Despite the great enthusiasm caused by the scientific community for the development of the method of back propagation of error, this also generated much debate about whether such training can actually be implemented in the brain. This was partly due to the fact that the signal return path mechanism was not obvious at that time, since there was no clear source of training and target signals. However, in 2006, several uncontrolled training procedures for neural networks with one or more layers were proposed using the so-called deep learning algorithms. These algorithms can be used to study intermediate representations, both with and without an output signal, in order to understand the main features of the distribution of sensory signals arriving at each layer of a neural network.

As in many other cases, tasks of high complexity require the use of not one, but several methods of solution or their synthesis. No exception and artificial neural networks. From the very beginning of this century, the works of various researchers have actively described neuro-fuzzy networks, cell-neural network models. Neural networks are also used, for example, to configure the parameters of fuzzy control systems. In general, there is no doubt about the further integration of artificial intelligence methods among themselves and with other methods of solving problems.


[link](https://neuronus.com/history/5-istoriya-nejronnykh-setej.html)